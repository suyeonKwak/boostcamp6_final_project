from utils import *
from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForSeq2SeqLM,
    TrainingArguments,
    EarlyStoppingCallback,
    Trainer,
)
import argparse
import nltk

nltk.download("punkt")


def train(model_name: str, model_dir: str = "./best_sum_model"):
    MODEL_NAME = model_name
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir="./huggingface")

    # load dataset
    train_dataset = load_data("./data/train.csv")
    valid_dataset = load_data("./data/valid.csv")

    y_train = train_dataset["summary"].values
    y_valid = valid_dataset["summary"].values

    # tokenizing dataset
    tokenized_train = tokenized_dataset(train_dataset, tokenizer)
    tokenized_valid = tokenized_dataset(valid_dataset, tokenizer)

    # make dataset for pytorch
    SUM_train_dataset = SUM_Dataset(tokenized_train, y_train)
    SUM_valid_dataset = SUM_Dataset(tokenized_valid, y_valid)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    print(device)

    # setting model hyperparameter
    sum_model_config = AutoConfig.from_pretrained(MODEL_NAME, cache_dir="./huggingface")

    sum_model = AutoModelForSeq2SeqLM.from_pretrained(
        MODEL_NAME, config=sum_model_config, cache_dir="./huggingface"
    )
    sum_model.to(device)

    training_args = TrainingArguments(
        output_dir="./results",
        seed=42,
        save_total_limit=1,
        save_strategy="epoch",
        # save_steps=500,
        logging_dir="./logs",
        logging_strategy="steps",
        logging_steps=1,
        num_train_epochs=10,
        learning_rate=5e-5,
        lr_scheduler_type="cosine",
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        warmup_steps=500,
        weight_decay=0.01,
        max_grad_norm=1.0,
        evaluation_strategy="epoch",
        eval_steps=500,
        load_best_model_at_end=True,
        metric_for_best_model="eval_micro f1 score",  # ìˆ˜ì • í•„ìš”
        greater_is_better=True,  # if loss False
    )

    early_stopping = EarlyStoppingCallback(
        early_stopping_patience=3,
    )

    # trainer
    trainer = Trainer(
        model=sum_model,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=SUM_train_dataset,  # training dataset
        eval_dataset=SUM_valid_dataset,  # evaluation dataset
        compute_metrics=compute_metrics,  # define metrics function
        callbacks=[early_stopping],
    )

    # train model
    trainer.train()
    sum_model.save(model_dir)


def inference(tokenizer_name: str, model_dir: str, text: str, fine_tuned: bool = False):

    Tokenizer_NAME = tokenizer_name
    MODEL_NAME = model_dir

    tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    if fine_tuned:
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
        model.to(device)
        model.eval()
    else:
        model = AutoModelForSeq2SeqLM.from_pretrained(Tokenizer_NAME)

    max_input_length = 512

    inputs = tokenizer(
        inputs, max_length=max_input_length, truncation=True, return_tensors="pt"
    )
    output = model.generate(
        **inputs, num_beams=8, do_sample=True, min_length=10, max_length=100
    )
    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]
    predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]

    print(predicted_title)


if __name__ == "__main__":

    parser = argparse.AugumentParser()
    parser.add_argument("--train", action="store_true", default=False)
    parser.add_argument("--fine_tuned", action="store_true", default=False)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument(
        "--model_name", type=str, default="lcw99/t5-base-korean-text-summary"
    )
    parser.add_argument("--model_dir", type=str, default="./best_sum_model/")
    args = parser.parse_args()

    set_seed(args.seed)

    # train
    if args.train:
        train(model_name=args.model_name, model_dir=args.model_dir)
    else:  # inference
        # text = input()
        text = """
        ì£¼ì¸ê³µ ê°•ì¸êµ¬(í•˜ì •ìš°)ëŠ” â€˜ìˆ˜ë¦¬ë‚¨ì—ì„œ í™ì–´ê°€ ë§ì´ ë‚˜ëŠ”ë° ë‹¤ ê°–ë‹¤ë²„ë¦°ë‹¤â€™ëŠ” ì¹œêµ¬ 
        ë°•ì‘ìˆ˜(í˜„ë´‰ì‹)ì˜ ì–˜ê¸°ë¥¼ ë“£ê³  ìˆ˜ë¦¬ë‚¨ì‚° í™ì–´ë¥¼ í•œêµ­ì— ìˆ˜ì¶œí•˜ê¸° ìœ„í•´ ìˆ˜ë¦¬ë‚¨ìœ¼ë¡œ ê°„ë‹¤. 
        êµ­ë¦½ìˆ˜ì‚°ê³¼í•™ì› ì¸¡ì€ â€œì‹¤ì œë¡œ ë‚¨ëŒ€ì„œì–‘ì— í™ì–´ê°€ ë§ì´ ì‚´ê³  ì•„ë¥´í—¨í‹°ë‚˜ë¥¼ ë¹„ë¡¯í•œ ë‚¨ë¯¸ êµ­ê°€ì—ì„œ í™ì–´ê°€ ë§ì´ ì¡íŒë‹¤â€ë©° 
        â€œìˆ˜ë¦¬ë‚¨ ì—°ì•ˆì—ë„ í™ì–´ê°€ ë§ì´ ì„œì‹í•  ê²ƒâ€ì´ë¼ê³  ì„¤ëª…í–ˆë‹¤.

        ê·¸ëŸ¬ë‚˜ ê´€ì„¸ì²­ì— ë”°ë¥´ë©´ í•œêµ­ì— ìˆ˜ë¦¬ë‚¨ì‚° í™ì–´ê°€ ìˆ˜ì…ëœ ì ì€ ì—†ë‹¤. 
        ì¼ê°ì—ì„  â€œëˆì„ ë²Œê¸° ìœ„í•´ ìˆ˜ë¦¬ë‚¨ì‚° í™ì–´ë¥¼ êµ¬í•˜ëŸ¬ ê°„ ì„¤ì •ì€ ê°œì—°ì„±ì´ ë–¨ì–´ì§„ë‹¤â€ëŠ” ì§€ì ë„ í•œë‹¤. 
        ë“œë¼ë§ˆ ë°°ê²½ì´ ëœ 2008~2010ë…„ì—ëŠ” ì´ë¯¸ êµ­ë‚´ì— ì•„ë¥´í—¨í‹°ë‚˜, ì¹ ë ˆ, ë¯¸êµ­ ë“± ì•„ë©”ë¦¬ì¹´ì‚° í™ì–´ê°€ ìˆ˜ì…ë˜ê³  ìˆì—ˆê¸° ë•Œë¬¸ì´ë‹¤. 
        ì‹¤ì œ ì¡°ë´‰í–‰ ì²´í¬ ì‘ì „ì— í˜‘ì¡°í–ˆë˜ â€˜í˜‘ë ¥ì Kì”¨â€™ë„ í™ì–´ ì‚¬ì—…ì´ ì•„ë‹ˆë¼ ìˆ˜ë¦¬ë‚¨ì— ì„ ë°•ìš© íŠ¹ìˆ˜ìš©ì ‘ë´‰ì„ íŒŒëŠ” ì‚¬ì—…ì„ í•˜ëŸ¬ ìˆ˜ë¦¬ë‚¨ì— ê°”ì—ˆë‹¤.
        """

        inference(
            tokenizer_name=args.model_name,
            model_dir=args.model_dir,
            text=text,
            fine_tuned=args.fine_tuned,
        )
